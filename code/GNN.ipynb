{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f807cb9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T08:33:02.236985Z",
     "iopub.status.busy": "2024-09-27T08:33:02.236604Z",
     "iopub.status.idle": "2024-09-27T08:33:16.915003Z",
     "shell.execute_reply": "2024-09-27T08:33:16.914055Z"
    },
    "id": "CTDjIkUy_a-A",
    "outputId": "376be9c1-9c1d-4eac-90eb-b71bc981c066",
    "papermill": {
     "duration": 14.686635,
     "end_time": "2024-09-27T08:33:16.917522",
     "exception": false,
     "start_time": "2024-09-27T08:33:02.230887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from six import iteritems\n",
    "from sklearn.metrics import (auc, f1_score, precision_recall_curve, roc_auc_score)\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow.compat.v1 as tf\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b20bce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T08:33:16.926870Z",
     "iopub.status.busy": "2024-09-27T08:33:16.926285Z",
     "iopub.status.idle": "2024-09-27T08:33:16.941084Z",
     "shell.execute_reply": "2024-09-27T08:33:16.940211Z"
    },
    "id": "KbkrXCD2_RrS",
    "papermill": {
     "duration": 0.021151,
     "end_time": "2024-09-27T08:33:16.943028",
     "exception": false,
     "start_time": "2024-09-27T08:33:16.921877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def walk(args):\n",
    "    walk_length, start, schema = args\n",
    "    # Simulate a random walk starting from start node.\n",
    "    rand = random.Random()\n",
    "\n",
    "    if schema:\n",
    "        schema_items = schema.split('-')\n",
    "        assert schema_items[0] == schema_items[-1]\n",
    "\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        candidates = []\n",
    "        for node in G[cur]:\n",
    "            if schema == '' or node_type[node] == schema_items[len(walk) % (len(schema_items) - 1)]:\n",
    "                candidates.append(node)\n",
    "        if candidates:\n",
    "            walk.append(rand.choice(candidates))\n",
    "        else:\n",
    "            break\n",
    "    return [str(node) for node in walk]\n",
    "\n",
    "def initializer(init_G, init_node_type):\n",
    "    global G\n",
    "    G = init_G\n",
    "    global node_type\n",
    "    node_type = init_node_type\n",
    "\n",
    "class RWGraph():\n",
    "    def __init__(self, nx_G, node_type_arr=None, num_workers=16):\n",
    "        self.G = nx_G\n",
    "        self.node_type = node_type_arr\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def node_list(self, nodes, num_walks):\n",
    "        for loop in range(num_walks):\n",
    "            for node in nodes:\n",
    "                yield node\n",
    "\n",
    "    def simulate_walks(self, num_walks, walk_length, schema=None):\n",
    "        all_walks = []\n",
    "        nodes = list(self.G.keys())\n",
    "        random.shuffle(nodes)\n",
    "\n",
    "        if schema is None:\n",
    "            with multiprocessing.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                all_walks = list(pool.imap(walk, ((walk_length, node, '') for node in tqdm(self.node_list(nodes, num_walks))), chunksize=256))\n",
    "        else:\n",
    "            schema_list = schema.split(',')\n",
    "            for schema_iter in schema_list:\n",
    "                with multiprocessing.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                    walks = list(pool.imap(walk, ((walk_length, node, schema_iter) for node in tqdm(self.node_list(nodes, num_walks)) if schema_iter.split('-')[0] == self.node_type[node]), chunksize=512))\n",
    "                all_walks.extend(walks)\n",
    "\n",
    "        return all_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9296c199",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T08:33:16.951001Z",
     "iopub.status.busy": "2024-09-27T08:33:16.950730Z",
     "iopub.status.idle": "2024-09-27T08:33:17.001219Z",
     "shell.execute_reply": "2024-09-27T08:33:17.000534Z"
    },
    "id": "5lVLLxK8_lzI",
    "papermill": {
     "duration": 0.056845,
     "end_time": "2024-09-27T08:33:17.003168",
     "exception": false,
     "start_time": "2024-09-27T08:33:16.946323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, count, index):\n",
    "        self.count = count\n",
    "        self.index = index\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--input', type=str, default='',\n",
    "                        help='Input dataset path')\n",
    "\n",
    "    parser.add_argument('--features', type=str, default=None,\n",
    "                        help='Input node features')\n",
    "\n",
    "    parser.add_argument('--walk-file', type=str, default=None,\n",
    "                        help='Input random walks')\n",
    "\n",
    "    parser.add_argument('--epoch', type=int, default=1,\n",
    "                        help='Number of epoch. Default is 100.')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=8,\n",
    "                        help='Number of batch_size. Default is 64.')\n",
    "\n",
    "    parser.add_argument('--eval-type', type=str, default='all',\n",
    "                        help='The edge type(s) for evaluation.')\n",
    "\n",
    "    parser.add_argument('--schema', type=str, default=None,\n",
    "                        help='The metapath schema (e.g., U-I-U,I-U-I).')\n",
    "\n",
    "    parser.add_argument('--dimensions', type=int, default=16,\n",
    "                        help='Number of dimensions. Default is 200.')\n",
    "\n",
    "    parser.add_argument('--edge-dim', type=int, default=4,\n",
    "                        help='Number of edge embedding dimensions. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--att-dim', type=int, default=4,\n",
    "                        help='Number of attention dimensions. Default is 20.')\n",
    "\n",
    "    parser.add_argument('--walk-length', type=int, default=5,\n",
    "                        help='Length of walk per source. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--num-walks', type=int, default=5,\n",
    "                        help='Number of walks per source. Default is 20.')\n",
    "\n",
    "    parser.add_argument('--window-size', type=int, default=5,\n",
    "                        help='Context size for optimization. Default is 5.')\n",
    "\n",
    "    parser.add_argument('--negative-samples', type=int, default=3,\n",
    "                        help='Negative samples for optimization. Default is 5.')\n",
    "\n",
    "    parser.add_argument('--neighbor-samples', type=int, default=1,\n",
    "                        help='Neighbor samples for aggregation. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--patience', type=int, default=5,\n",
    "                        help='Early stopping patience. Default is 5.')\n",
    "\n",
    "    parser.add_argument('--num-workers', type=int, default=1,\n",
    "                        help='Number of workers for generating random walks. Default is 16.')\n",
    "    parser.add_argument(\"-f\", \"--file\", required=False)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_G_from_edges(edges):\n",
    "    edge_dict = defaultdict(set)\n",
    "    for edge in edges:\n",
    "        u, v = str(edge[0]), str(edge[1])\n",
    "        edge_dict[u].add(v)\n",
    "        edge_dict[v].add(u)\n",
    "    return edge_dict\n",
    "\n",
    "def load_training_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    edge_data_by_type = dict()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            if words[0] not in edge_data_by_type:\n",
    "                edge_data_by_type[words[0]] = list()\n",
    "            x, y = words[1], words[2]\n",
    "            edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    print('Total training nodes: ' + str(len(all_nodes)))\n",
    "    return edge_data_by_type\n",
    "\n",
    "\n",
    "def load_testing_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    true_edge_data_by_type = dict()\n",
    "    false_edge_data_by_type = dict()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            x, y = words[1], words[2]\n",
    "            if int(words[3]) == 1:\n",
    "                if words[0] not in true_edge_data_by_type:\n",
    "                    true_edge_data_by_type[words[0]] = list()\n",
    "                true_edge_data_by_type[words[0]].append((x, y))\n",
    "            else:\n",
    "                if words[0] not in false_edge_data_by_type:\n",
    "                    false_edge_data_by_type[words[0]] = list()\n",
    "                false_edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    return true_edge_data_by_type, false_edge_data_by_type\n",
    "\n",
    "def load_node_type(f_name):\n",
    "    print('We are loading node type from:', f_name)\n",
    "    node_type = {}\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            node_type[items[0]] = items[1]\n",
    "    return node_type\n",
    "\n",
    "def load_feature_data(f_name):\n",
    "    feature_dic = {}\n",
    "    with open(f_name, 'r') as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            items = line.strip().split()\n",
    "            feature_dic[items[0]] = items[1:]\n",
    "    return feature_dic\n",
    "\n",
    "def generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers):\n",
    "    if schema is not None:\n",
    "        node_type = load_node_type('/kaggle/input/ddi-dataset/DDI/data5/node_type.txt')\n",
    "    else:\n",
    "        node_type = None\n",
    "\n",
    "    all_walks = []\n",
    "    for layer_id, layer_name in enumerate(network_data):\n",
    "        tmp_data = network_data[layer_name]\n",
    "        # start to do the random walk on a layer\n",
    "\n",
    "        layer_walker = RWGraph(get_G_from_edges(tmp_data), node_type, num_workers)\n",
    "        print('Generating random walks for layer', layer_id)\n",
    "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, schema=schema)\n",
    "\n",
    "        all_walks.append(layer_walks)\n",
    "\n",
    "    print('Finish generating the walks')\n",
    "\n",
    "    return all_walks\n",
    "\n",
    "def generate_pairs(all_walks, vocab, window_size, num_workers):\n",
    "    pairs = []\n",
    "    skip_window = window_size // 2\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        print('Generating training pairs for layer', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for i in range(len(walk)):\n",
    "                for j in range(1, skip_window + 1):\n",
    "                    if i - j >= 0:\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))\n",
    "                    if i + j < len(walk):\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))\n",
    "    return pairs\n",
    "\n",
    "def generate_vocab(all_walks):\n",
    "    index2word = []\n",
    "    raw_vocab = defaultdict(int)\n",
    "\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        print('Counting vocab for layer', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for word in walk:\n",
    "                raw_vocab[word] += 1\n",
    "\n",
    "    vocab = {}\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "        index2word.append(word)\n",
    "\n",
    "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n",
    "    for i, word in enumerate(index2word):\n",
    "        vocab[word].index = i\n",
    "\n",
    "    return vocab, index2word\n",
    "\n",
    "def load_walks(walk_file):\n",
    "    print('Loading walks')\n",
    "    all_walks = []\n",
    "    with open(walk_file, 'r') as f:\n",
    "        for line in f:\n",
    "            content = line.strip().split()\n",
    "            layer_id = int(content[0])\n",
    "            if layer_id >= len(all_walks):\n",
    "                all_walks.append([])\n",
    "            all_walks[layer_id].append(content[1:])\n",
    "    return all_walks\n",
    "\n",
    "def save_walks(walk_file, all_walks):\n",
    "    with open(walk_file, 'w') as f:\n",
    "        for layer_id, walks in enumerate(all_walks):\n",
    "            print('Saving walks for layer', layer_id)\n",
    "            for walk in tqdm(walks):\n",
    "                f.write(' '.join([str(layer_id)] + [str(x) for x in walk]) + '\\n')\n",
    "\n",
    "def generate(network_data, num_walks, walk_length, schema, file_name, window_size, num_workers, walk_file):\n",
    "    if walk_file is not None:\n",
    "        all_walks = load_walks(walk_file)\n",
    "    else:\n",
    "        all_walks = generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers)\n",
    "        save_walks('/walks.txt', all_walks)\n",
    "    vocab, index2word = generate_vocab(all_walks)\n",
    "    train_pairs = generate_pairs(all_walks, vocab, window_size, num_workers)\n",
    "\n",
    "    return vocab, index2word, train_pairs\n",
    "\n",
    "def generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples):\n",
    "    edge_type_count = len(edge_types)\n",
    "    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n",
    "    for r in range(edge_type_count):\n",
    "        print('Generating neighbors for layer', r)\n",
    "        g = network_data[edge_types[r]]\n",
    "        for (x, y) in tqdm(g):\n",
    "            ix = vocab[x].index\n",
    "            iy = vocab[y].index\n",
    "            neighbors[ix][r].append(iy)\n",
    "            neighbors[iy][r].append(ix)\n",
    "        for i in range(num_nodes):\n",
    "            if len(neighbors[i][r]) == 0:\n",
    "                neighbors[i][r] = [i] * neighbor_samples\n",
    "            elif len(neighbors[i][r]) < neighbor_samples:\n",
    "                neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "            elif len(neighbors[i][r]) > neighbor_samples:\n",
    "                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))\n",
    "    return neighbors\n",
    "\n",
    "def get_score(local_model, node1, node2):\n",
    "    try:\n",
    "        vector1 = local_model[node1]\n",
    "        vector2 = local_model[node2]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "def evaluate(model, true_edges, false_edges):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    true_num = 0\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(1)\n",
    "            prediction_list.append(tmp_score)\n",
    "            true_num += 1\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(0)\n",
    "            prediction_list.append(tmp_score)\n",
    "\n",
    "    sorted_pred = prediction_list[:]\n",
    "    sorted_pred.sort()\n",
    "    threshold = sorted_pred[-true_num]\n",
    "\n",
    "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] >= threshold:\n",
    "            y_pred[i] = 1\n",
    "\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf98b188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T08:33:17.011236Z",
     "iopub.status.busy": "2024-09-27T08:33:17.010948Z",
     "iopub.status.idle": "2024-09-27T08:33:17.070639Z",
     "shell.execute_reply": "2024-09-27T08:33:17.069075Z"
    },
    "id": "oOVaIdDP_4fg",
    "outputId": "44e7cdc3-7643-4a21-cb94-c11fce856e80",
    "papermill": {
     "duration": 0.065713,
     "end_time": "2024-09-27T08:33:17.072196",
     "exception": true,
     "start_time": "2024-09-27T08:33:17.006483",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input INPUT] [--features FEATURES]\n",
      "                             [--walk-file WALK_FILE] [--epoch EPOCH]\n",
      "                             [--batch-size BATCH_SIZE] [--eval-type EVAL_TYPE]\n",
      "                             [--schema SCHEMA] [--dimensions DIMENSIONS]\n",
      "                             [--edge-dim EDGE_DIM] [--att-dim ATT_DIM]\n",
      "                             [--walk-length WALK_LENGTH]\n",
      "                             [--num-walks NUM_WALKS]\n",
      "                             [--window-size WINDOW_SIZE]\n",
      "                             [--negative-samples NEGATIVE_SAMPLES]\n",
      "                             [--neighbor-samples NEIGHBOR_SAMPLES]\n",
      "                             [--patience PATIENCE] [--num-workers NUM_WORKERS]\n",
      "                             [-f FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --HistoryManager.hist_file=:memory:\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def get_batches(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield (np.array(x).astype(np.int32), np.array(y).reshape(-1, 1).astype(np.int32), np.array(t).astype(np.int32), np.array(neigh).astype(np.int32))\n",
    "\n",
    "def train_model(network_data, feature_dic, log_name, f_num, file_name):\n",
    "    vocab, index2word, train_pairs = generate(network_data, args.num_walks, args.walk_length, args.schema, file_name, args.window_size, args.num_workers, args.walk_file)\n",
    "\n",
    "    edge_types = list(network_data.keys())\n",
    "\n",
    "    num_nodes = len(index2word)\n",
    "    edge_type_count = len(edge_types)\n",
    "    epochs = args.epoch\n",
    "    batch_size = args.batch_size\n",
    "    embedding_size = args.dimensions # Dimension of the embedding vector.\n",
    "    embedding_u_size = args.edge_dim\n",
    "    u_num = edge_type_count\n",
    "    num_sampled = args.negative_samples # Number of negative examples to sample.\n",
    "    dim_a = args.att_dim\n",
    "    att_head = 1\n",
    "    neighbor_samples = args.neighbor_samples\n",
    "\n",
    "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
    "\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    if feature_dic is not None:\n",
    "        feature_dim = len(list(feature_dic.values())[0])\n",
    "        print('feature dimension: ' + str(feature_dim))\n",
    "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
    "        for key, value in feature_dic.items():\n",
    "            if key in vocab:\n",
    "                features[vocab[key].index, :] = np.array(value)\n",
    "\n",
    "    with graph.as_default():\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        if feature_dic is not None:\n",
    "            node_features = tf.Variable(features, name='node_features', trainable=False)\n",
    "            feature_weights = tf.Variable(tf.truncated_normal([feature_dim, embedding_size], stddev=1.0))\n",
    "\n",
    "            embed_trans = tf.Variable(tf.truncated_normal([feature_dim, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "            u_embed_trans = tf.Variable(tf.truncated_normal([edge_type_count, feature_dim, embedding_u_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        else:\n",
    "            node_embeddings = tf.Variable(tf.random_uniform([num_nodes, embedding_size], -1.0, 1.0))\n",
    "            node_type_embeddings = tf.Variable(tf.random_uniform([num_nodes, u_num, embedding_u_size], -1.0, 1.0))\n",
    "\n",
    "        trans_weights = tf.Variable(tf.truncated_normal([edge_type_count, embedding_u_size, embedding_size // att_head], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        trans_weights_s1 = tf.Variable(tf.truncated_normal([edge_type_count, embedding_u_size, dim_a], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        trans_weights_s2 = tf.Variable(tf.truncated_normal([edge_type_count, dim_a, att_head], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([num_nodes]))\n",
    "\n",
    "        # Input data\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "        train_types = tf.placeholder(tf.int32, shape=[None])\n",
    "        node_neigh = tf.placeholder(tf.int32, shape=[None, edge_type_count, neighbor_samples])\n",
    "\n",
    "        # Look up embeddings for nodes\n",
    "        if feature_dic is not None:\n",
    "            node_embed = tf.nn.embedding_lookup(node_features, train_inputs)\n",
    "            node_embed = tf.matmul(node_embed, embed_trans)\n",
    "        else:\n",
    "            node_embed = tf.nn.embedding_lookup(node_embeddings, train_inputs)\n",
    "\n",
    "        if feature_dic is not None:\n",
    "            node_embed_neighbors = tf.nn.embedding_lookup(node_features, node_neigh)\n",
    "            node_embed_tmp = tf.concat([tf.matmul(tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, 0], [-1, 1, -1, -1]), [-1, feature_dim]), tf.reshape(tf.slice(u_embed_trans, [i, 0, 0], [1, -1, -1]), [feature_dim, embedding_u_size])) for i in range(edge_type_count)], axis=0)\n",
    "            node_type_embed = tf.transpose(tf.reduce_mean(tf.reshape(node_embed_tmp, [edge_type_count, -1, neighbor_samples, embedding_u_size]), axis=2), perm=[1,0,2])\n",
    "        else:\n",
    "            node_embed_neighbors = tf.nn.embedding_lookup(node_type_embeddings, node_neigh)\n",
    "            node_embed_tmp = tf.concat([tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, i, 0], [-1, 1, -1, 1, -1]), [1, -1, neighbor_samples, embedding_u_size]) for i in range(edge_type_count)], axis=0)\n",
    "            node_type_embed = tf.transpose(tf.reduce_mean(node_embed_tmp, axis=2), perm=[1,0,2])\n",
    "\n",
    "        trans_w = tf.nn.embedding_lookup(trans_weights, train_types)\n",
    "        trans_w_s1 = tf.nn.embedding_lookup(trans_weights_s1, train_types)\n",
    "        trans_w_s2 = tf.nn.embedding_lookup(trans_weights_s2, train_types)\n",
    "\n",
    "        attention = tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.tanh(tf.matmul(node_type_embed, trans_w_s1)), trans_w_s2), [-1, u_num])), [-1, att_head, u_num])\n",
    "        node_type_embed = tf.matmul(attention, node_type_embed)\n",
    "        node_embed = node_embed + tf.reshape(tf.matmul(node_type_embed, trans_w), [-1, embedding_size])\n",
    "\n",
    "        if feature_dic is not None:\n",
    "            node_feat = tf.nn.embedding_lookup(node_features, train_inputs)\n",
    "            node_embed = node_embed + tf.matmul(node_feat, feature_weights)\n",
    "\n",
    "        last_node_embed = tf.nn.l2_normalize(node_embed, axis=1)\n",
    "\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=last_node_embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=num_nodes))\n",
    "        plot_loss = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n",
    "\n",
    "        # Add ops to save and restore all the variables.\n",
    "        # saver = tf.train.Saver(max_to_keep=20)\n",
    "\n",
    "        merged = tf.summary.merge_all(key=tf.GraphKeys.SUMMARIES)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    # Launch the graph\n",
    "    print(\"Optimizing\")\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        writer = tf.summary.FileWriter(\"./runs/\" + log_name, sess.graph) # tensorboard --logdir=./runs\n",
    "        sess.run(init)\n",
    "\n",
    "        print('Training')\n",
    "        g_iter = 0\n",
    "        best_score = 0\n",
    "        test_score = (0.0, 0.0, 0.0)\n",
    "        patience = 0\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(train_pairs)\n",
    "            batches = get_batches(train_pairs, neighbors, batch_size)\n",
    "\n",
    "            data_iter = tqdm(batches,\n",
    "                            desc=\"epoch %d\" % (epoch),\n",
    "                            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
    "                            bar_format=\"{l_bar}{r_bar}\")\n",
    "            avg_loss = 0.0\n",
    "\n",
    "            for i, data in enumerate(data_iter):\n",
    "                feed_dict = {train_inputs: data[0], train_labels: data[1], train_types: data[2], node_neigh: data[3]}\n",
    "                _, loss_value, summary_str = sess.run([optimizer, loss, merged], feed_dict)\n",
    "                writer.add_summary(summary_str, g_iter)\n",
    "\n",
    "                g_iter += 1\n",
    "\n",
    "                avg_loss += loss_value\n",
    "\n",
    "                if i % 5000 == 0:\n",
    "                    post_fix = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"iter\": i,\n",
    "                        \"avg_loss\": avg_loss / (i + 1),\n",
    "                        \"loss\": loss_value\n",
    "                    }\n",
    "                    data_iter.write(str(post_fix))\n",
    "\n",
    "            final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n",
    "            for i in range(edge_type_count):\n",
    "                for j in range(num_nodes):\n",
    "                    final_model[edge_types[i]][index2word[j]] = np.array(sess.run(last_node_embed, {train_inputs: [j], train_types: [i], node_neigh: [neighbors[j]]})[0])\n",
    "\n",
    "            valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "            test_aucs, test_f1s, test_prs = [], [], []\n",
    "            for i in range(edge_type_count):\n",
    "                if args.eval_type == 'all' or edge_types[i] in args.eval_type.split(','):\n",
    "                    tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], valid_true_data_by_edge[edge_types[i]], valid_false_data_by_edge[edge_types[i]])\n",
    "                    valid_aucs.append(tmp_auc)\n",
    "                    valid_f1s.append(tmp_f1)\n",
    "                    valid_prs.append(tmp_pr)\n",
    "\n",
    "                    tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], testing_true_data_by_edge[edge_types[i]], testing_false_data_by_edge[edge_types[i]])\n",
    "                    test_aucs.append(tmp_auc)\n",
    "                    test_f1s.append(tmp_f1)\n",
    "                    test_prs.append(tmp_pr)\n",
    "            print('valid auc:', np.mean(valid_aucs))\n",
    "            print('valid pr:', np.mean(valid_prs))\n",
    "            print('valid f1:', np.mean(valid_f1s))\n",
    "\n",
    "            average_auc = np.mean(test_aucs)\n",
    "            average_f1 = np.mean(test_f1s)\n",
    "            average_pr = np.mean(test_prs)\n",
    "\n",
    "            cur_score = np.mean(valid_aucs)\n",
    "            if cur_score > best_score:\n",
    "                best_score = cur_score\n",
    "                test_score = (average_auc, average_f1, average_pr)\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience > args.patience:\n",
    "                    print('Early Stopping')\n",
    "                    break\n",
    "        final_modelss=[]\n",
    "        for i in range(edge_type_count):\n",
    "          for j in range(num_nodes):\n",
    "            final_modelss.append([edge_types[i],index2word[j],final_model[edge_types[i]][index2word[j]]])\n",
    "        df = pd.DataFrame((final_modelss))\n",
    "        df.to_csv('/final_modelss'+str(f_num)+'_d_'+str(embedding_size)+'.csv', header=None, index=None) #, sep=' '\n",
    "        # df = pd.DataFrame((final_model))\n",
    "        # df.to_csv(file_name+'/final_model'+str(f_num)+'.csv', header=None, index=None) #, sep=' '\n",
    "        # with open('GFG.csv', 'w') as f:\n",
    "        #   write = csv.writer(f)\n",
    "        #   for i in range(edge_type_count):\n",
    "        #     write.writerows(final_model[edge_types[i]])\n",
    "        # df = pd.DataFrame((index2word))\n",
    "        # df.to_csv('/content/drive/MyDrive/DDI/index2word.csv', header=None, index=None) #, sep=' '\n",
    "    return test_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    file_name = args.input\n",
    "    print(args)\n",
    "    if args.features is not None:\n",
    "        feature_dic = load_feature_data(args.features)\n",
    "        f_num = str(args.features[-5])\n",
    "    else:\n",
    "        feature_dic = None\n",
    "        f_num = str(\"dr\")\n",
    "    log_name = file_name.split('/')[-1] + f'_evaltype_{args.eval_type}_b_{args.batch_size}_e_{args.epoch}'\n",
    "\n",
    "    training_data_by_type = load_training_data( '/kaggle/input/ddi-dataset/DDI/data5/train.txt')\n",
    "    valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data('/kaggle/input/ddi-dataset/DDI/data5/valid.txt')\n",
    "    testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data('/kaggle/input/ddi-dataset/DDI/data5/test.txt')\n",
    "\n",
    "    average_auc, average_f1, average_pr = train_model(training_data_by_type, feature_dic, log_name + '_' + time.strftime('%Y-%m-%d %H-%M-%S',time.localtime(time.time())),f_num,file_name)\n",
    "\n",
    "    print('Overall ROC-AUC:', average_auc)\n",
    "    print('Overall PR-AUC', average_pr)\n",
    "    print('Overall F1:', average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fd133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-26T10:11:31.946698Z",
     "iopub.status.busy": "2024-09-26T10:11:31.945900Z",
     "iopub.status.idle": "2024-09-26T10:11:32.525267Z",
     "shell.execute_reply": "2024-09-26T10:11:32.524370Z",
     "shell.execute_reply.started": "2024-09-26T10:11:31.946654Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Loss data\n",
    "iterations = [0, 5000, 10000, 15000, 20000, 25000, 30000, 35000]\n",
    "avg_losses = [14.08, 9.57, 7.84, 6.76, 6.03, 5.49, 5.06, 4.71]\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics = {\n",
    "    'Valid AUC': 0.816,\n",
    "    'Valid PR': 0.803,\n",
    "    'Valid F1': 0.777,\n",
    "    'Overall ROC-AUC': 0.782,\n",
    "    'Overall PR-AUC': 0.773,\n",
    "    'Overall F1': 0.729\n",
    "}\n",
    "\n",
    "# Plotting the Loss Curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iterations, avg_losses, marker='o', color='b')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.ylim([0, 15])\n",
    "plt.xticks(iterations)\n",
    "plt.grid()\n",
    "\n",
    "# Plotting Evaluation Metrics\n",
    "plt.subplot(1, 2, 2)\n",
    "names = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "\n",
    "plt.barh(names, values, color='skyblue')\n",
    "plt.title('Validation and Overall Metrics')\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel('Score')\n",
    "plt.grid(axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa37df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5765904,
     "sourceId": 9479543,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.277992,
   "end_time": "2024-09-27T08:33:18.698244",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-27T08:32:59.420252",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
